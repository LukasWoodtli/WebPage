{
    "componentChunkName": "component---src-templates-blog-post-tsx",
    "path": "/blog/language_implementation_patterns",
    "result": {"data":{"site":{"siteMetadata":{"title":"My personal Website"}},"markdownRemark":{"id":"6bbd4217-bc21-53cb-be9e-32aaca14c2c5","excerpt":"This page collects notes and citations from the book: Language Implementation Patterns by Terence Parr Some of my examples can be found here [TOC] Getting…","html":"<p>This page collects notes and citations from the book:</p>\n<p><a href=\"https://pragprog.com/book/tpdsl/language-implementation-patterns\">Language Implementation Patterns by Terence Parr</a></p>\n<p>Some of my examples can be found <a href=\"https://github.com/LukasWoodtli/LanguageImplementationPatterns\">here</a></p>\n<p>[TOC]</p>\n<h1 id=\"getting-started-with-parsing\">Getting Started with Parsing</h1>\n<h2 id=\"basic-parsing-patterns\">Basic Parsing Patterns</h2>\n<p><em>“The act of recognizing a phrase by computer is called <strong>parsing</strong>.”</em></p>\n<p><em>“You can think of grammars as functional specifications or design documents for parsers.”</em></p>\n<p><em>“Grammars are more than designs, though. They are actually executable ‘programs’ written in a domain-specific language (DSL) specifically designed for expressing language structures.”</em></p>\n<h3 id=\"identifying-phrase-structure\">Identifying Phrase Structure</h3>\n<p><em>“Vocabulary symbols (tokens) play different roles like variable and operator. We can even identify the role of token subsequences like expression.”</em></p>\n<p><em>“Parse trees are important because they tell us everything we need to know about the syntax (structure) of a phrase.”</em></p>\n<p><em>“To parse [is to generate] a two-dimensional parse tree from a flat token sequence.”</em></p>\n<h3 id=\"building-recursive-descent-parsers\">Building Recursive-Descent Parsers</h3>\n<p><em>“A parser checks whether a sentence conforms to the syntax of a language.”</em></p>\n<p><em>“A language is just a set of valid sentences.”</em></p>\n<p><em>“A top-down parser […] starts at the top of the parse tree and works its way down to the token leaf nodes.”</em></p>\n<p><em>“Recursive-Descent Parser</em></p>\n<ul>\n<li><em><strong>Descent</strong> refers to its top-down nature, and</em></li>\n<li><em><strong>Recursive</strong> refers to the fact that its functions potentially call themselves.”</em></li>\n</ul>\n<p><em>“Nesting in a parse tree begets [(Kind zeugen)] recursion in a recursive-descent parser.”</em></p>\n<p><em>“LL(1)</em></p>\n<ul>\n<li><em>The first <strong>L</strong> means ‘read the input from left to right.’</em></li>\n<li><em>The second <strong>L</strong> means ‘descend into parse tree children from left to right.‘”</em></li>\n</ul>\n<h3 id=\"parser-construction-using-a-grammar-dsl\">Parser Construction Using a Grammar DSL</h3>\n<p><em>“[We can use] a DSL specifically designed for describing languages. ‘Programs’ in this DSL are called <strong>grammars</strong>. Tools that translate grammars to parsers are called <strong>parser generators</strong>.”</em></p>\n<p><em>“Grammars are […] functional specifications for languages.”</em></p>\n<p><em>“Substructures in the parse tree and functions in the parser correspond to <strong>rules</strong> in a grammar.”</em></p>\n<p><em>“[Lexer:] combining input characters into vocabulary symbols (tokens).”</em></p>\n<h3 id=\"tokenizing-sentences\">Tokenizing Sentences</h3>\n<p><em>“Recognizers that feed off character streams are called <strong>tokenizers</strong> or <strong>lexers</strong>”</em></p>\n<p><em>“At the character level, we refer to syntax as the <strong>lexical structure</strong>.”</em></p>\n<p><em>“Grammars describe language structures, and so we can also use them for lexical specifications.”</em></p>\n<p><em>“Lexical rules start with an uppercase letter”</em></p>\n<p><em>“Lexer and parser design patterns […] are nearly identical. […] The only difference lies in the type of their input symbols, characters or tokens.”</em></p>\n<h3 id=\"pattern-1-mapping-grammars-to-recursive-descent-recognizers\">Pattern 1: Mapping Grammars to Recursive-Descent Recognizers</h3>\n<p><em>“Even when building lexers and parsers by hand, the best starting point is a grammar.”</em></p>\n<p><em>”<strong>Left recursion</strong> results in an infinite method invocation loop.”</em></p>\n<p><em>“The following rule yields a parser that does not terminate:</em>”</p>\n<div class=\"gatsby-highlight\" data-language=\"antlr4\"><pre class=\"language-antlr4\"><code class=\"language-antlr4\"><span class=\"token definition rule class-name\">r</span> <span class=\"token punctuation\">:</span> r <span class=\"token constant\">X</span> <span class=\"token punctuation\">;</span></code></pre></div>\n<p><em>“we’d end up with a function that immediately called itself”</em></p>\n<div class=\"gatsby-highlight\" data-language=\"java\"><pre class=\"language-java\"><code class=\"language-java\"><span class=\"token keyword\">void</span> <span class=\"token function\">r</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span> <span class=\"token function\">r</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token function\">match</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">X</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token punctuation\">}</span></code></pre></div>\n<p><em>“Besides left-recursive rules, there are other grammar constructs that yield <strong>nondeterministic</strong> recursive-descent recognizers. A nondeterministic recognizer cannot decide which path to take.”</em></p>\n<p><em>“A grammar, G, is a set of rules from which we generate a class definition (in any object-oriented programming language) containing a method for each rule”</em></p>\n<p><em>“For each rule, r, defined in a grammar, we build a method of the same name”</em></p>\n<p><em>“Token references for token type <code class=\"language-text\">T</code> become calls to <code class=\"language-text\">match(T)</code>. match is a support method in Parser that consumes a token if <code class=\"language-text\">T</code> is the current lookahead token. If there is a mismatch, match throws an exception.”</em></p>\n<h4 id=\"alternatives\">Alternatives</h4>\n<p><em>“Alternatives become either a switch or an if-then-else sequence”</em></p>\n<p><img src=\"/a81a493f098a9655537d3718c055baf5/alternatives.svg\" alt=\"Alternatives\"></p>\n<div class=\"gatsby-highlight\" data-language=\"java\"><pre class=\"language-java\"><code class=\"language-java\"><span class=\"token keyword\">switch</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">&lt;</span>lookahead<span class=\"token operator\">-</span>token<span class=\"token operator\">></span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">case</span> <span class=\"token operator\">&lt;</span>token1<span class=\"token operator\">-</span>predicting<span class=\"token operator\">-</span>alt1<span class=\"token operator\">></span><span class=\"token operator\">:</span>\n  <span class=\"token keyword\">case</span> <span class=\"token operator\">&lt;</span>token2<span class=\"token operator\">-</span>predicting<span class=\"token operator\">-</span>alt1<span class=\"token operator\">></span><span class=\"token operator\">:</span>\n    <span class=\"token operator\">&lt;</span>match<span class=\"token operator\">-</span>alt1<span class=\"token operator\">></span>\n    <span class=\"token keyword\">break</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">case</span> <span class=\"token operator\">&lt;</span>token1<span class=\"token operator\">-</span>predicting<span class=\"token operator\">-</span>alt2<span class=\"token operator\">></span><span class=\"token operator\">:</span>\n  <span class=\"token keyword\">case</span> <span class=\"token operator\">&lt;</span>token2<span class=\"token operator\">-</span>predicting<span class=\"token operator\">-</span>alt2<span class=\"token operator\">></span><span class=\"token operator\">:</span>\n    <span class=\"token operator\">&lt;</span>match<span class=\"token operator\">-</span>alt2<span class=\"token operator\">></span>\n    <span class=\"token keyword\">break</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">case</span> <span class=\"token operator\">&lt;</span>token1<span class=\"token operator\">-</span>predicting<span class=\"token operator\">-</span>altN<span class=\"token operator\">></span><span class=\"token operator\">:</span>\n  <span class=\"token keyword\">case</span> <span class=\"token operator\">&lt;</span>token2<span class=\"token operator\">-</span>predicting<span class=\"token operator\">-</span>altN<span class=\"token operator\">></span><span class=\"token operator\">:</span>\n    <span class=\"token operator\">&lt;</span>match<span class=\"token operator\">-</span>altN<span class=\"token operator\">></span>\n    <span class=\"token keyword\">break</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">default</span><span class=\"token operator\">:</span>\n    <span class=\"token operator\">&lt;</span><span class=\"token keyword\">throw</span><span class=\"token operator\">-</span>exception<span class=\"token operator\">></span>\n  <span class=\"token punctuation\">}</span></code></pre></div>\n<h4 id=\"optional-subrule-t\">Optional subrule <code class=\"language-text\">(T)?</code></h4>\n<p><img src=\"/7fc285724ac4ed7d013eb5ae145c9375/option.svg\" alt=\"Option\"></p>\n<div class=\"gatsby-highlight\" data-language=\"java\"><pre class=\"language-java\"><code class=\"language-java\"><span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">&lt;</span>lookahead<span class=\"token operator\">-</span>is<span class=\"token operator\">-</span><span class=\"token class-name\">T</span><span class=\"token operator\">></span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span> <span class=\"token function\">match</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">T</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span class=\"token punctuation\">}</span> <span class=\"token comment\">// no error else clause</span></code></pre></div>\n<h4 id=\"one-or-more--subrules\">One or more <code class=\"language-text\">(...)+</code> subrules</h4>\n<p><img src=\"/8e2b5b4e6a78c72965d7dc83ea2fbe0e/one_or_more.svg\" alt=\"One or more\"></p>\n<div class=\"gatsby-highlight\" data-language=\"java\"><pre class=\"language-java\"><code class=\"language-java\"><span class=\"token keyword\">do</span> <span class=\"token punctuation\">{</span> <span class=\"token operator\">&lt;</span>code<span class=\"token operator\">-</span>matching<span class=\"token operator\">-</span>alternatives<span class=\"token operator\">></span> <span class=\"token punctuation\">}</span>\n<span class=\"token keyword\">while</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&lt;</span>lookahead<span class=\"token operator\">-</span>predicts<span class=\"token operator\">-</span>an<span class=\"token operator\">-</span>alt<span class=\"token operator\">-</span>of<span class=\"token operator\">-</span>subrule<span class=\"token operator\">></span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<h4 id=\"zero-or-more--subrules\">Zero or more <code class=\"language-text\">(...)*</code> subrules</h4>\n<p><img src=\"/808dab78fa8cd3b2e19d68824bd5092f/zero_or_more.svg\" alt=\"Zero or more\"></p>\n<div class=\"gatsby-highlight\" data-language=\"java\"><pre class=\"language-java\"><code class=\"language-java\"><span class=\"token keyword\">while</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&lt;</span>lookahead<span class=\"token operator\">-</span>predicts<span class=\"token operator\">-</span>an<span class=\"token operator\">-</span>alt<span class=\"token operator\">-</span>of<span class=\"token operator\">-</span>subrule<span class=\"token operator\">></span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token operator\">&lt;</span>code<span class=\"token operator\">-</span>matching<span class=\"token operator\">-</span>alternatives<span class=\"token operator\">></span> <span class=\"token punctuation\">}</span></code></pre></div>\n<h3 id=\"pattern-2-ll1-recursive-descent-lexer\">Pattern 2: LL(1) Recursive-Descent Lexer</h3>\n<p><em>“Lexers derive a stream of tokens from a character stream by recognizing lexical patterns. Lexers are also called <strong>scanners</strong>, <strong>lexical analyzers</strong>, and <strong>tokenizers</strong>.”</em></p>\n<p><em>“This pattern can recognize nested lexical structures such as nested comments”</em></p>\n<p><em>“The goal of the lexer is to emit a sequence of tokens. Each token has two primary attributes: a <strong>token type</strong> (symbol category) and the text associated with it.”</em></p>\n<h3 id=\"pattern-3-ll1-recursive-descent-parser\">Pattern 3: LL(1) Recursive-Descent Parser</h3>\n<p><em>“It’s the weakest form of recursive-descent parser but the easiest to understand and implement.”</em></p>\n<p><em>“To make parsing decisions, the parser tests the current lookahead token against the alternatives’ lookahead sets. A lookahead set is the set of tokens that can begin a particular alternative.”</em></p>\n<p><em>“Formally, we compute lookahead sets using two computations: FIRST and FOLLOW. In practice, though, it’s easier to simply ask ourselves, ‘What tokens can possibly start phrases beginning at this alternative?‘”</em></p>\n<h4 id=\"deterministic-parsing-decisions\">Deterministic Parsing Decisions</h4>\n<p><em>“LL parsing decisions work only when the lookahead sets predicting the alternatives are disjoint”</em></p>\n<p><em>“If the lookahead sets overlap, though, the parser is <strong>nondeterministic</strong>-it cannot determine which alternative to choose.”</em></p>\n<p><em>“Building an LL(1) parser is the easiest way to learn about parsers. In practice, though, we really need more than a single token of lookahead.”</em></p>\n<h3 id=\"pattern-4-llk-recursive-descent-parser\">Pattern 4: LL(k) Recursive-Descent Parser</h3>\n<p><em>“The strength of a recursive-descent parser depends entirely on the strength of its lookahead decisions.”</em></p>\n<p><em>“Having more lookahead is like being able to see farther down multiple paths emanating from a fork in a maze. The farther we can see ahead, the easier it is to decide which path to take. More powerful parsing decisions make it easier to build parsers.”</em></p>\n<p><em>“For example, we want to recognize input such as <code class=\"language-text\">[a, b=c, [d,e]]</code>”</em></p>\n<div class=\"gatsby-highlight\" data-language=\"antlr4\"><pre class=\"language-antlr4\"><code class=\"language-antlr4\"><span class=\"token definition rule class-name\">list</span> <span class=\"token punctuation\">:</span> <span class=\"token string\">'['</span> elements <span class=\"token string\">']'</span> <span class=\"token punctuation\">;</span>\n<span class=\"token definition rule class-name\">elements</span> <span class=\"token punctuation\">:</span> element <span class=\"token punctuation\">(</span> <span class=\"token string\">','</span> element<span class=\"token punctuation\">)</span><span class=\"token operator\">*</span> <span class=\"token punctuation\">;</span>\n<span class=\"token definition rule class-name\">element</span> <span class=\"token punctuation\">:</span> <span class=\"token constant\">NAME</span> <span class=\"token string\">'='</span> <span class=\"token constant\">NAME</span>\n        <span class=\"token operator\">|</span> <span class=\"token constant\">NAME</span>\n        <span class=\"token operator\">|</span> list\n        <span class=\"token punctuation\">;</span></code></pre></div>\n<p><em>”<code class=\"language-text\">element</code> [is] non-LL(1) since the first two alternatives start with the same <code class=\"language-text\">NAME</code> token.”</em></p>\n<p><em>“The lookahead depth <strong>k</strong> in LL(k) is really a maximum not the exact, fixed amount of lookahead each parsing decision uses.”</em></p>\n<h2 id=\"enhanced-parsing-patterns\">Enhanced Parsing Patterns</h2>\n<h3 id=\"parsing-with-arbitrary-lookahead\">Parsing with Arbitrary Lookahead</h3>\n<p><em>“[Some] language constructs […] only differ on the right side. For example, C++ function definitions and declarations are identical until the parser sees <code class=\"language-text\">;</code> or <code class=\"language-text\">{</code> :”</em></p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">void</span> <span class=\"token function\">bar</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">}</span> <span class=\"token comment\">// a function definition</span>\n<span class=\"token keyword\">void</span> <span class=\"token function\">bar</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token comment\">// a function declaration</span></code></pre></div>\n<p><em>“function headers can be arbitrarily long, the distinguishing token does not appear at a fixed lookahead position from the left side of the statement.”</em></p>\n<p><em>“The parser can speculatively parse as far ahead as it needs.”</em></p>\n<p><em>“Speculatively matching the alternatives of a rule effectively orders them. The first alternative that matches wins. This is great because we can use ordering to specify precedence.”</em></p>\n<p><em>“With ordered alternatives, there is no ambiguity because the parser consistently chooses the first of two ambiguous alternatives. By having the parser pay attention to the order of alternatives”</em></p>\n<p><em>“Though speculative parsing has a lot of advantages, there are two drawbacks. First, it can make debugging more difficult. When the parser speculatively parses ahead, it’s easy to get lost with all of the scanning ahead and rewinding. Second, backtracking can be extremely slow. Fortunately, we can fix the efficiency issue.”</em></p>\n<h3 id=\"parsing-like-a-pack-rat\">Parsing like a Pack Rat</h3>\n<p><em>“Almost by definition, we use backtracking parsers only when we need to distinguish between similar language constructs. If the constructs are similar, the associated grammar likely contains repeated references to the same rule.”</em></p>\n<h3 id=\"directing-the-parse-with-semantic-information\">Directing the Parse with Semantic Information</h3>\n<p><em>“The parsers we’re working with in this book recognize <strong>context-free languages</strong>. A context-free language is a language whose constructs don’t depend on the presence of other constructs.\nUnfortunately, some programming languages have context-sensitive phrases. To handle context-sensitive phrases with a context-free parser, we have to predicate alternatives. In effect, a predicate is just a run-time boolean test that says when it’s OK to match an alternative. The predicates gate the associated alternatives in and out.”</em></p>\n<p><em>“In C++, the expression <code class=\"language-text\">T(6)</code> is either a function call or a constructor-style typecast depending on whether <code class=\"language-text\">T</code> is a function or type name. A C++ parser literally does not know how to interpret <code class=\"language-text\">T(6)</code> without seeing the definition of <code class=\"language-text\">T</code>. Such a construct is <strong>context sensitive</strong> and, in this case, ambiguous from a purely syntactic point of view.”</em></p>\n<p><em>“Ambiguous grammars lead to <strong>nondeterministic parsers</strong>”</em></p>\n<h3 id=\"pattern-5-backtracking-parser\">Pattern 5 Backtracking Parser</h3>\n<p><em>“This pattern adds speculative parsing support (arbitrary lookahead) to any recursive-descent recognizer.”</em></p>\n<p><em>”[…] we can’t map all grammars to recursive-descent parsers. Only <strong>non-left-recursive grammars</strong> work (no rule can directly or indirectly invoke itself without consuming a token).”</em></p>\n<p><em>”[…] we can’t always get properly functioning (deterministic) parsers even from non-left-recursive grammars. The problem is that fixed lookahead LL parsers need the lookahead sets predicting alternatives to be disjoint.”</em></p>\n<p><em>“This pattern overcomes this lookahead issue by allowing arbitrary lookahead […]. To look arbitrarily ahead, we need infrastructure to support backtracking. Backtracking also gives us a way to specify the precedence of ambiguous rule alternatives (alternatives that can match the same input). Backtracking parsers, by definition, try the alternatives in order.”</em></p>\n<p><em>“Syntactic predicates are grammar fragments that specify the lookahead language predicting an alternative.”</em></p>\n<p><em>“ANTLR’s notion of grammars plus syntactic predicates [are called] Parsing Expression Grammars (PEGs)”</em></p>\n<p><em>“In the functional language world, syntactic predicates are called <strong>parser combinators</strong>”</em></p>\n<p><em>“Syntactic predicates and speculative parsing are extremely useful when parsing phrases that look the same from the left edge. Distinguishing between C++ function definitions and declarations is a prime example”</em></p>\n<h4 id=\"dealing-with-actions-while-backtracking\">Dealing with Actions While Backtracking</h4>\n<p><em>“Either we disallow actions or disallow actions with side effects, or we parse winning alternatives twice.”</em></p>\n<p><em>“During speculation, all actions are off. Once the parser knows an alternative will match, however, it can match the alternative again “with feeling” to do the actions.”</em></p>\n<h3 id=\"pattern-6-memoizing-parser\">Pattern 6 Memoizing Parser</h3>\n<p><em>“This pattern records partial parsing results during backtracking to guarantee linear parsing performance, at the cost of a small amount of memory.”</em></p>\n<p><em>“Memoizing is a form of dynamic programming”</em></p>\n<p><em>“Another name for <strong>memoizing recursive-descent parser</strong> is <strong>packrat parser</strong>”</em></p>\n<p><em>“Memoization only helps us, though, if we invoke the same rule at the same input position more than once.”</em></p>\n<p><em>“For example, upon input <code class=\"language-text\">(3+4);</code>, a backtracking parser derived from the following rule invokes <code class=\"language-text\">expr</code> twice:”</em></p>\n<div class=\"gatsby-highlight\" data-language=\"antlr4\"><pre class=\"language-antlr4\"><code class=\"language-antlr4\"><span class=\"token definition rule class-name\">s</span> <span class=\"token punctuation\">:</span> expr <span class=\"token string\">'!'</span> <span class=\"token comment\">// assume backtracking parser tries this alternative</span>\n  <span class=\"token operator\">|</span> expr <span class=\"token string\">';'</span> <span class=\"token comment\">// and then this one</span>\n  <span class=\"token punctuation\">;</span>\n<span class=\"token definition rule class-name\">expr</span> <span class=\"token punctuation\">:</span> <span class=\"token operator\">..</span>. <span class=\"token punctuation\">;</span> <span class=\"token comment\">// match input such as \"(3+4)\"</span></code></pre></div>\n<p><em>“Rule <code class=\"language-text\">s</code> invokes <code class=\"language-text\">expr</code> to speculatively match the first alternative. <code class=\"language-text\">expr</code> succeeds, but <code class=\"language-text\">s</code> finds that the next\ninput symbol is <code class=\"language-text\">;</code> and not <code class=\"language-text\">!</code>. Rule <code class=\"language-text\">s</code> rewinds the input and tries the second alternative. The parser immediately calls\n<code class=\"language-text\">expr</code> again and at the same input position. […] To avoid reparsing, all we have to do is remember that <code class=\"language-text\">expr</code> succeeded\nthe last time we tried it at this position.”</em></p>\n<p><em>“Packrat parsers are guaranteed to have linear performance [and also] linear space complexity.”</em></p>\n<h3 id=\"pattern-7-predicated-parser\">Pattern 7 Predicated Parser</h3>\n<p><em>“This pattern augments any top-down parser with arbitrary boolean expressions that help make parsing decisions.”</em></p>\n<p><em>“These boolean expressions are called semantic predicates and specify the semantic applicability of an alternative.\nPredicates that evaluate to false effectively “turn off” a parser decision path. From a grammar point of view, false\npredicates make alternatives invisible.”</em></p>\n<p><em>“We need semantic predicates when the parser cannot use syntax alone to make parsing decisions, that is, when the parser\ncannot distinguish between alternatives without using run-time information. The most common case is when we need to use symbol\ntable information to guide the parse.”</em></p>\n<p><em>“Predicates are also useful when a parser must recognize multiple versions of an input language. For example, the GCC C\ncompiler adds a number of extensions beyond C. Java 5.0 introduced the enum keyword to support enumerated types.”</em></p>\n<h1 id=\"analyzing-languages\">Analyzing Languages</h1>\n<h2 id=\"chapter-4-building-intermediate-form-trees\">Chapter 4 Building Intermediate Form Trees</h2>\n<p><em>“Only the simplest language applications get away with reading\ninput and directly generating output. Such applications are called\n<strong>syntax-directed</strong> applications”</em></p>\n<p><em>“The key characteristic of syntax-directed applications is that\nthey translate input phrase by phrase using a single pass over\nthe input.”</em></p>\n<p><em>“Most language applications, however, need to build an\n<strong>intermediate representation</strong> (IR) or intermediate form.”</em></p>\n<p><em>“The goal of an application’s reader component is to fill an IR\ndata structure with elements of interest from the input stream.”</em></p>\n<p><em>“to get a computer to understand a nontrivial sentence, we have to\nbreak it down into a series of operations and operands.”</em></p>\n<p><em>“Once we identify the operators and operands among the input\ntokens, we need to build an IR data structure. For most language\napplications, that means building a tree data structure. In\nparticular, we’ll build an <strong>abstract syntax tree</strong> (AST).”</em></p>\n<p><em>“Abstract syntax tree (AST) […] hold the key tokens from the input\nstream and record grammatical relationships discovered during the parse.”</em></p>\n<p><em>“ASTs are the <strong>lingua franca</strong> spoken by the various stages in a\nlanguage application. Each stage performs a computation, rewrites\nthe tree, or creates another data structure before passing the tree\nalong to the next stage.”</em></p>\n<p><em>“The four most common IR tree patterns:”</em></p>\n<ul>\n<li><em>”<strong>Parse trees</strong> record how a parser recognizes an input sentence. The interior nodes are rule names, and the leaves are tokens. Although parse trees are less suitable than ASTs for most language applications, parsers can create them automatically.”</em></li>\n<li><em>”<strong>Homogeneous AST</strong> […]: If all the nodes have the same type, we say that they are homogeneous. With a single node type, there can be no specialty fields to reference child subtrees. Nodes track children with lists of child pointers.”</em></li>\n<li><em>”<strong>Normalized Heterogeneous AST</strong> […]: Trees with a multitude of node types are called heterogeneous trees. Normalized het erogeneous trees use a normalized list of children like homogeneous trees.”</em></li>\n<li><em>”<strong>Irregular Heterogeneous AST</strong> […]: When we refer to an AST as heterogeneous, we also assume that the nodes have irregular children. Instead of a normalized child list, the nodes have named fields, one per child.”</em></li>\n</ul>\n<h3 id=\"why-we-build-trees\">Why We Build Trees</h3>\n<p><em>“Many languages have subphrases and nested structures.”</em></p>\n<p><em>“trees are the perfect data structure to represent ordered and\nnested structures. There are two general kinds of trees we’re going\nto look at: <strong>parse trees</strong> and <strong>abstract syntax trees</strong>.”</em></p>\n<p><em>“Parse trees record the sequence of rules a parser applies as well\nas the tokens it matches. Interior parse tree nodes represent rule\napplications, and leaf nodes represent token matches.”</em></p>\n<p><em>“a parser execution trace isn’t really the best IR. Certainly we\nneed to pinpoint the various substructures, but we don’t need to\nname them explicitly.”</em></p>\n<h3 id=\"building-abstract-syntax-trees\">Building Abstract Syntax Trees</h3>\n<p><em>“An IR tree should be the following:</em></p>\n<ul>\n<li><em><strong>Dense</strong>: No unnecessary nodes</em></li>\n<li><em><strong>Convenient</strong>: Easy to walk</em></li>\n<li><em><strong>Meaningful</strong>: Emphasize operators, operands, and the relationship between them rather than artifacts from the grammar”</em></li>\n</ul>\n<p><em>“it should be easy and fast to identify patterns in the tree.\nLanguage applications that use intermediate trees usually make\nmultiple passes over the trees in order to analyze or build other\ndata structures. The structure of intermediate trees should be\nbrain-dead simple.”</em></p>\n<p><em>“the tree structure should be insensitive to changes in the\ngrammar”</em></p>\n<p><em>“Computers only care about operators and operands.”</em></p>\n<p><em>“By condensing the input to its essential elements, we decouple it\nfrom the original syntax. So, for example, assignment syntax boils\ndown to an assignment operator and two operands. Decoupling does two\nthings. First, it gets us closer to the operator-operand model of\nthe CPU. Second, we can have different languages share a common\nintermediate form.”</em></p>\n<p><em>“The key idea behind AST structure is that tokens representing\noperators or operations become subtree roots. All other tokens\nbecome operands (children of operator nodes).”</em></p>\n<h3 id=\"representing-pseudo-operations-in-asts\">Representing Pseudo-operations in ASTs</h3>\n<p><em>“Not all programming language constructs map directly to\nexecutable code.”</em></p>\n<p><em>“In some cases, there is no reasonable input token to use as a\nsubtree root. We must invent an <strong>imaginary token</strong>, a token for\nwhich there is no corresponding input token. For example, variable\ndeclarations in languages derived from C usually need an imaginary\ntoken.”</em></p>\n<h3 id=\"implementing-asts-in-java\">Implementing ASTs in Java</h3>\n<p><em>“technically, we need only one data type: a generic tree node with\na list of children. What we really care about is the tree structure\n(relationships between nodes), not necessarily the node\nimplementation type(s) themselves.”</em></p>\n<p><em>“Trees built from a single data type are called\n<strong>homogeneous trees</strong>.”</em></p>\n<h3 id=\"enforcing-tree-structure-with-the-type-system\">Enforcing Tree Structure with the Type System</h3>\n<p><em>“To avoid creating improperly structured ASTs, we can co-opt the implementation\nlanguage’s static type system to enforce structure.”</em></p>\n<p><em>“The best way to create ASTs and to verify their structure is with a formal mechanism.”</em></p>\n<h3 id=\"constructing-asts-with-antlr-grammars\">Constructing ASTs with ANTLR Grammars</h3>\n<p><em>” The key is that we are declaring what the AST should look like, not how to build it.\nIt is analogous to using a grammar to specify syntax rather than building a parser.”</em></p>\n<p><em>“We looked at two different ways to structure intermediate representations (parse trees and ASTs)\nand three different ways to implement ASTs.”</em></p>\n<ul>\n<li><em>“Pattern 8, Parse Tree […]:</em>\n<ul>\n<li><em>Pros: Parser generators can automatically build these for us.</em></li>\n<li><em>Cons: Parse trees are full of noise (unnecessary nodes). They are sensitive to changes in the grammar unrelated to syntax. If a parser generator generates heterogeneous node types, there can be literally hundreds of class definitions.</em>”</li>\n</ul>\n</li>\n<li><em>“Pattern 9, Homogeneous AST […]:</em>\n<ul>\n<li><em>Pros: Homogeneous trees are very simple.</em></li>\n<li><em>Cons: It’s cumbersome to annotate AST nodes because the single node type has the union of all needed fields. There is no way to add methods specific to a particular kind of node.”</em></li>\n</ul>\n</li>\n<li><em>“Pattern 10, Normalized Heterogeneous AST […]:</em>\n<ul>\n<li><em>Pros: It’s easy to add operator or operand-specific data and methods.</em></li>\n<li><em>Cons: Large grammars like Java’s need about 200 class definitions to be fully heterogeneous. That’s a lot of files to read and write.”</em></li>\n</ul>\n</li>\n<li><em>“Pattern 11, Irregular Heterogeneous AST […].</em>\n<ul>\n<li><em>Pros: It’s easy to add operator- or operand-specific data and methods. Sometimes code operating on nodes is more readable because the children (operands) have names rather than positions like <code class=\"language-text\">children[0]</code>. Building tree-walking methods for a small set of heterogeneous nodes is quick and easy.</em></li>\n<li><em>Cons: As with Pattern 10, Normalized Heterogeneous AST […], there are lots of AST classes to read and write. Having irregular children makes building external visitors difficult. Most of the time we have to build tree walkers by hand using Pattern 12, Embedded Heterogeneous Tree Walker […]”</em></li>\n</ul>\n</li>\n</ul>\n<p><em>“If you’re in doubt about which is best in your situation, choosing Pattern 10, Normalized Heterogeneous AST […]\nis a safe bet.”</em></p>\n<h3 id=\"pattern-8-parse-tree\">Pattern 8: Parse Tree</h3>\n<h4 id=\"purpose\">Purpose</h4>\n<p><em>“A parse tree describes how a parser recognized an input sentence.\nA parse tree is sometimes called a syntax tree (as opposed to an abstract\nsyntax tree). Despite not being that useful for building interpreters and\ntranslators [they] are heavily used\nby development environments and text rewriting systems.”</em></p>\n<h4 id=\"discussion\">Discussion</h4>\n<p><em>“Parse trees record the sequence of rules a parser applies as well as the tokens\nit matches. Interior parse tree nodes represent rule applications, and leaf\nnodes represent token matches.”</em></p>\n<p><em>“Parse trees are really easy to build by hand and are so regular that tools like\nANTLR can automate the process for us.”</em></p>\n<p><em>“Parse\ntrees are full of noise because of all the interior rule nodes. They are also very\nsensitive to changes in the grammar.”</em></p>\n<p><em>“An AST captures just the essential information from the input: all of the input\ntokens and the appropriate structure. The interior nodes are operators or\noperations rather than rule names.”</em></p>\n<p><em>“Parse trees mirror the function call graphs of a recursive-descent parser”</em></p>\n<h3 id=\"pattern-9-homogeneous-ast\">Pattern 9: Homogeneous AST</h3>\n<h4 id=\"purpose-1\">Purpose</h4>\n<p><em>“A homogeneous tree implements an abstract syntax tree (AST) using a single\nnode data type and a normalized child list representation.”</em></p>\n<h4 id=\"discussion-1\">Discussion</h4>\n<p><em>“The key idea behind an AST is the operator-operand tree structure, not the\nnode data type.”</em></p>\n<p><em>“We don’t need to use the type system of our implementation language to distinguish\nbetween nodes.”</em></p>\n<p><em>“In fact, homogeneous ASTs are the only\nconvenient choice for non-object-oriented languages like C.”</em></p>\n<p><em>“Homogeneous ASTs necessarily use a normalized child representation:\n<code class=\"language-text\">List&lt;AST></code>. This makes it particularly easy to build external visitors.”</em></p>\n<h3 id=\"pattern-10-normalized-heterogeneous-ast\">Pattern 10: Normalized Heterogeneous AST</h3>\n<h4 id=\"purpose-2\">Purpose</h4>\n<p><em>“This pattern implements an abstract syntax tree (AST) using more than a single\nnode data type but with a normalized child list representation.”</em></p>\n<h4 id=\"discussion-2\">Discussion</h4>\n<p><em>“This pattern makes the most sense when we need to store node-specific data”</em></p>\n<p><em>“The normalized child list makes it much easier to build external visitors.”</em></p>\n<h3 id=\"pattern-11-irregular-heterogeneous-ast\">Pattern 11: Irregular Heterogeneous AST</h3>\n<h4 id=\"purpose-3\">Purpose</h4>\n<p><em>“This pattern implements an abstract syntax tree (AST) using more than a single\nnode data type and with an irregular child list representation.”</em></p>\n<h4 id=\"discussion-3\">Discussion</h4>\n<p><em>“Instead of a uniform list of children, each node data type has specific\n(named) child fields. In this sense, the child pointers are irregular.\nIn some cases, named fields lead to more readable code.”</em></p>\n<p><em>“It’s very natural to name the fields of a class, in this case naming the children of\na node. The big downside to using nodes with irregular children is that it’s\nmuch less convenient to build tree walkers”</em></p>\n<h2 id=\"walking-and-rewriting-trees\">Walking and Rewriting Trees</h2>\n<p><em>“Tree walking is one of the key processes going on in a large language application.”</em></p>\n<p><em>“In real applications, though, tree walking gets surprisingly complicat- ed. There are a number of different variations, sometimes even within the same application.”</em></p>\n<p><em>“The variation we choose depends on whether we have the source code for our tree nodes, whether the trees have normalized children, whether the trees are homogeneous or heterogeneous, whether we need to rewrite trees while walking, and even in which order we need to walk the nodes.”</em></p>\n<h3 id=\"walking-trees-and-visitation-order\">Walking Trees and Visitation Order</h3>\n<ul>\n<li><em>“Preorder traversal or top-down traversal: <span class=\"math math-inline\"><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>+</mo><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">+ 1 2</annotation></semantics></math></span></span>. Visit a (parent) node before visiting its children.</em></li>\n<li><em>Inorder traversal: <span class=\"math math-inline\"><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn><mo>+</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1 + 2</annotation></semantics></math></span></span>. Visit a node in between visiting children.</em></li>\n<li><em>Postorder traversal or bottom-up traversal: <span class=\"math math-inline\"><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>12</mn><mo>+</mo></mrow><annotation encoding=\"application/x-tex\">1 2 +</annotation></semantics></math></span></span>. Visit a node after visiting its children.”</em></li>\n</ul>\n<h3 id=\"pattern-12-embedded-heterogeneous-tree-walker\">Pattern 12: Embedded Heterogeneous Tree Walker</h3>\n<h4 id=\"purpose-4\">Purpose</h4>\n<p><em>“This pattern walks heterogeneous ASTs using a set of recursive methods defined within the node class definitions.”</em></p>\n<h4 id=\"discussion-4\">Discussion</h4>\n<p><em>“This is the easiest tree-walking pattern to understand, but, ultimately, this approach doesn’t scale well. Because it distributes tree-walking code across all node definitions, it works best when there are only a few node definitions.”</em></p>\n<h3 id=\"pattern-13-external-tree-visitor\">Pattern 13: External Tree Visitor</h3>\n<h4 id=\"purpose-5\">Purpose</h4>\n<p><em>“This pattern encapsulates all tree-walking code associated with a particular task into a single visitor class.\nVisitors combine tree walking and action execution code outside the AST node definitions. Consequently, we can change the functionality of the tree walker without having to change the AST class definitions and can even switch visitors on the fly. An external visitor can walk either heterogeneous or homogeneous AST nodes.”</em></p>\n<h4 id=\"discussion-5\">Discussion</h4>\n<p><em>“The visitor pattern is the workhorse of choice for tree walking in most language applications. Ultimately you might get tired of manually building visitors, though”</em></p>\n<h4 id=\"implementation\">Implementation</h4>\n<p><em>“There are two ways to implement this pattern. The first is more traditional and relies on the node types themselves. The second relies on the node’s token type instead.”</em></p>\n<h5 id=\"visitor-switching-on-node-type\">Visitor Switching on Node Type</h5>\n<p><em>“The traditional implementation of the visitor pattern originally specified in\n‘Design Patterns: Elements of Reusable Object-Oriented Software’\nrelies on a ‘double-dispatch’ method within each AST node. The double-dispatch method redirects <code class=\"language-text\">visit()</code>\ncalls on a node to an appropriate method in a visitor servicing that node type. The visitor is like a set of callback methods.”</em></p>\n<h5 id=\"switching-on-the-token-type-to-build-independent-visitors\">Switching on the Token Type to Build Independent Visitors</h5>\n<p><em>“For language applications, we build trees from tokens. Since we can distinguish between tokens using the token type, we can also\ndistinguish between AST nodes using the token type. By switching on the token type rather than the AST node type, we can avoid\nthe <code class=\"language-text\">visit()</code> method in each AST node. In its place, we use just one dispatch method inside the visitor.”</em></p>\n<h3 id=\"pattern-14-tree-grammar\">Pattern 14: Tree Grammar</h3>\n<h4 id=\"purpose-6\">Purpose</h4>\n<p><em>“Tree grammars are a terse and formal way of building an external visitor.”</em></p>\n<h4 id=\"discussion-6\">Discussion</h4>\n<p><em>“Tree grammars look just like conventional parser grammars except that we can match subtree patterns as well.\nAs with parser grammars, we can embed actions to extract information or reorganize the input (a tree, in this case).”</em></p>\n<p><em>“ANTLR generates tree walkers from tree grammars that literally act like parsers.”</em></p>\n<p><em>“Tree grammars do not care about the implementation language classes used to represent AST nodes (they work with both homogeneous and heterogeneous AST nodes).”</em></p>\n<h3 id=\"pattern-15-tree-pattern-matcher\">Pattern 15: Tree Pattern Matcher</h3>\n<h4 id=\"purpose-7\">Purpose</h4>\n<p><em>“This pattern walks trees, triggering actions or tree rewrites as it encounters tree patterns of interest.\nThe process of matching and rewriting trees is formally called term rewriting.”</em></p>\n<h4 id=\"discussion-7\">Discussion</h4>\n<p><em>“Using a tree pattern matcher differs from using a tree grammar in two important ways:</em></p>\n<ul>\n<li><em>We have to specify patterns only for the subtrees we care about.</em></li>\n<li><em>We don’t need to direct the tree walk.”</em></li>\n</ul>\n<p><em>“A tree pattern matcher is analogous to text rewriting tools such as <code class=\"language-text\">awk</code>, <code class=\"language-text\">sed</code>, and <code class=\"language-text\">perl</code>.”</em></p>\n<h2 id=\"chapter-6-tracking-and-identifying-program-symbols\">Chapter 6 Tracking and Identifying Program Symbols</h2>\n<h3 id=\"collecting-information-about-program-entities\">Collecting Information About Program Entities</h3>\n<p><em>“what we need to know for each symbol”</em></p>\n<ul>\n<li><em>Name</em></li>\n<li><em>Category</em>: class, method, variable, label…</li>\n<li><em>Type</em></li>\n</ul>\n<h3 id=\"grouping-symbols-into-scopes\">Grouping Symbols into Scopes</h3>\n<ul>\n<li><em>Static vs. dynamic scoping</em></li>\n<li><em>Nesting</em></li>\n<li><em>Contents</em>: declarations, statements or both</li>\n<li><em>Visibility</em></li>\n</ul>\n<h3 id=\"resolving-symbols\">Resolving Symbols</h3>\n<p><em>“to resolve a symbol reference, we look for it in its semantic context, starting with the current scope. If <code class=\"language-text\">resolve()</code> doesn’t find the symbol in the current scope, it asks the enclosing scope if it can find the symbol. <code class=\"language-text\">resolve()</code> recursively walks toward the root of the scope tree until it finds the symbol or runs out of scopes.”</em></p>\n<h2 id=\"chapter-7-managing-symbol-tables-for-data-aggregates\">Chapter 7 Managing Symbol Tables for Data Aggregates</h2>\n<h3 id=\"building-scope-trees-for-structs\">Building Scope Trees for Structs</h3>\n<p><em>“We also have to resolve symbols within struct scopes from the outside. In other words, a language application might have to figure out which field <code class=\"language-text\">«expr».x</code> refers to.”</em></p>\n<p><em>“The general (recursive) rule for resolving <code class=\"language-text\">«expr».x</code> is to determine the type of <code class=\"language-text\">«expr»</code> and then look up <code class=\"language-text\">x</code> in that scope.”</em></p>\n<p><em>“we need two different resolve methods: one for looking up isolated symbols like <code class=\"language-text\">d</code> and another for resolving member access expressions like <code class=\"language-text\">d.i</code>.”</em></p>\n<h3 id=\"building-scope-trees-for-classes\">Building Scope Trees for Classes</h3>\n<p><em>“Per the usual object-oriented language conventions, […] we want to look up the inheritance chain before looking in the global scope.”</em></p>\n<p><em>“To handle […] forward references […]. We can make two passes over the input, one to define symbols and another to resolve them”</em></p>\n<h2 id=\"chapter-8-enforcing-static-typing-rules\">Chapter 8 Enforcing Static Typing Rules</h2>\n<p><em>“Sometimes, though, we write code that make no sense even if the syntax is correct. Such programs violate a language’s semantic rules.”</em></p>\n<p><em>“Languages typically have lots and lots of semantic rules. Some rules are run-time constraints (dynamic semantics), and some are compile-time constraints (static semantics).”</em></p>\n<p><em>“Some languages enforce the same rule statically and then again dynamically to guard against hostile programs. For example, Java does type checking at compile-time as well as at run-time.”</em></p>\n<h1 id=\"building-interpreters\">Building Interpreters</h1>\n<h2 id=\"chapter-9-building-high-level-interpreters\">Chapter 9 Building High-Level Interpreters</h2>\n<p><em>“High-level interpreters directly execute source code instructions or the AST equivalent. (Low-level interpreters execute instructions called bytecodes that are close to CPU machine instructions.)”</em></p>\n<p><em>“An interpreter simulates an idealized computer in software. Such “computers” have a processor, code memory, data memory, and (usually) a stack. The processor pulls instructions from the code memory, decodes them, and exe- cutes them. An instruction can read or write to the data memory or onto the stack. Function calls save return addresses so they can return to the instruction following the function call.”</em></p>\n<p><em>“There are three things to consider when building an interpreter: how to store data, how and when to track symbols, and how to execute instructions.”</em></p>\n<h3 id=\"designing-high-level-interpreter-memory-systems\">Designing High-Level Interpreter Memory Systems</h3>\n<p><em>“High-level interpreters store values according to variable names, not memory addresses (like low-level interpreters and CPUs do). That means we’ve got to represent memory with a dictionary mapping names to values. There are three kinds of memory spaces to worry about for most programming languages: global memory, function spaces (for parameters and locals), and data aggregate instances (structs or objects).”</em></p>\n<h3 id=\"processing-instructions\">Processing Instructions</h3>\n<p><em>“fetch-decode-execute cycle: First, we load an instruction from code memory. Then, we decode the instruction to find the operation and operands. Finally, we execute the operation. Rinse and repeat.”</em></p>\n<h2 id=\"chapter-10-building-bytecode-interpreters\">Chapter 10 Building Bytecode Interpreters</h2>\n<h3 id=\"bytecode-machine-architecture\">Bytecode Machine Architecture</h3>\n<p><em>“A bytecode interpreter simulates a computer with the following components:”</em></p>\n<ul>\n<li><em>Code memory</em>: the assembled bytecode (instructions and operands)</li>\n<li><em>ip register</em>: pointing at the next instruction to be executed</li>\n<li><em>Global memory</em>: space for variables</li>\n<li><em>CPU</em>: executes instructions</li>\n<li><em>Constant pool</em>: holds the constants (known at assembly time)</li>\n<li><em>Function call stack</em>: for parameters, local variables and return values</li>\n<li><em>fp register</em>: points to the top of the function call stack</li>\n</ul>\n<p>In addition a Stack-Based interpreter has:</p>\n<ul>\n<li><em>Operand stack</em>: for temporary values (instead of registers)</li>\n<li><em>sp register</em>: points to the top of the operand stack</li>\n</ul>\n<p>A Register-Based interpreter has (instead of a operand stack):</p>\n<ul>\n<li><em>infinite register set per function call</em>: parameters, local variables and return values</li>\n</ul>\n<h1 id=\"translating-and-generating-languages\">Translating and Generating Languages</h1>\n<h2 id=\"translating-computer-languages\">Translating Computer Languages</h2>\n<p><em>“Translation involves fully understanding each input phrase, picking\nan appropriate output construct, and filling it with elements from\nthe input model.”</em></p>\n<p><em>“We usually have to create an input model like an AST because we\ncan’t always do semantic analysis properly as we parse.”</em></p>\n<p><em>“According to the needs of the task at hand, we compute everything we\nneed to know about the input and then make a decision about mapping an\ninput phrase to an output phrase.”</em></p>\n<h3 id=\"rule-based-translation\">Rule-Based Translation</h3>\n<p><em>“Rule-based systems are particularly good at legacy code conversions\nbecause we want the translated code to be natural.”</em></p>\n<p><em>“These rule engines let us say what to do, not how to do it.\nThey are powerful, implementation-language independent, expressive,\nformal, and beautiful.”</em></p>\n<p><em>“These systems tend to be complex beasts. With a large number of\ntranslation rules, translations can be slow.”</em></p>\n<p><em>“The rule engines themselves are black boxes, which can\nmake it hard to understand what’s gone wrong in a translation.\nSome of these systems were also designed to be the center of the\nuniverse, making them tricky to integrate into applications.”</em></p>\n<h3 id=\"model-driven-translation\">Model-Driven Translation</h3>\n<p><em>“In a model-driven translator, everything centers around an input\nmodel created by the parser.”</em></p>\n<p><em>“From the AST input model, we’re going to learn how to derive an\nappropriate output model instead of immediately generating output.”</em></p>\n<p><em>“As we walk the input model, we’ll match subtrees and create\noutput objects to represent translated phrases”</em></p>\n<p><em>“Using a hierarchy of output objects to represent the output makes\nsense because it’s equivalent to a syntax tree.”</em></p>\n<h3 id=\"decoupling-input-model-traversal-from-output-order\">Decoupling Input Model Traversal from Output Order</h3>\n<p><em>“There are two ways to solve input-output ordering mismatches. First,\nwe can walk the tree [multiple times].”</em></p>\n<p><em>“This works but is inefficient because we have to walk the\n(potentially very large) tree [multiple times]. This is an\noutput-driven approach because it ‘pulls’ information\nfrom the input model according to the output order.”</em></p>\n<p><em>“The second choice is to walk the input model a single time,\ncollecting the declarations and definitions in lists instead of\ndirectly emitting text”</em></p>\n<p><em>“This input-driven approach lets us decouple the input and output\norder, albeit at the cost of buffering up the various output pieces.\nIt’s totally worth it, though.”</em></p>\n<p><em>“We know that the translation process is about creating an input\nmodel, enriching it with semantic information, and then creating an\nappropriate output model. Target-specific generator classes\nare familiar and well-structured, but building them is a lot of work,\nand they’re often a hassle to use. Visitors that generate text directly\nare much more convenient. Unfortunately, print statements lock the\norder of the output elements to the input model traversal order.\nBesides, computing output strings in a general-purpose programming\nlanguage ain’t exactly pretty.”</em></p>\n<h3 id=\"organizing-translated-phrases-into-a-nested-model\">Organizing Translated Phrases into a Nested Model</h3>\n<p><em>“In general, translators need to track a few locations\nin the output model. These locations are typically things such as\nthe current file object, the current class object, or the current method\nobject.”</em></p>\n<h3 id=\"pattern-29-syntax-directed-translator\">Pattern 29: Syntax-Directed Translator</h3>\n<h4 id=\"purpose-8\">Purpose</h4>\n<p><em>“This pattern generates text using a grammar, or equivalent hand-built parser,\nembedded with actions.”</em></p>\n<h4 id=\"discussion-8\">Discussion</h4>\n<p><em>“Syntax-directed translators are little more than grammars salted with actions\n(code snippets). They don’t build internal models and then walk them to\ngenerate output.\nPutting actions directly in a grammar sometimes makes the grammar difficult\nto read. It also locks the grammar into a single bit of translator functionality.”</em></p>\n<h3 id=\"pattern-30-rule-based-translator\">Pattern 30: Rule-Based Translator</h3>\n<h4 id=\"purpose-9\">Purpose</h4>\n<p><em>“A rule-based translator expresses a translation with a set of ‘x becomes y’\nrules, written in the DSL of a pattern-matching engine.”</em></p>\n<h4 id=\"discussion-9\">Discussion</h4>\n<p><em>“To use a rule-based system, we have to feed it two things: a grammar that\ndescribes input sentences and a set of translation rules.”</em></p>\n<p><em>“Although it looks like we’re doing text-to-text transformations,\nthe underlying engine is actually doing tree rewrites.\nFor complicated translators, we need to build a lot of ancillary data structures\nbeyond trees such as symbol tables and control-flow graphs.”</em></p>\n<h3 id=\"pattern-31-target-specific-generator-classes\">Pattern 31: Target-Specific Generator Classes</h3>\n<h4 id=\"purpose-10\">Purpose</h4>\n<p><em>“This pattern describes a class library whose sole purpose is to represent and\ngenerate output constructs in a particular language.”</em></p>\n<h4 id=\"discussion-10\">Discussion</h4>\n<p><em>“Rather than use print statements to generate programs or data, generator\nclasses let us insulate application code from the exact syntax of an output\nlanguage. […] we’re defining a special class for each output element.”</em></p>","frontmatter":{"title":"Language Implementation Patterns","tags":["Computer Science","Parsing","Design Patterns"],"category":"Programming"},"headings":[{"depth":1,"value":"Getting Started with Parsing"},{"depth":2,"value":"Basic Parsing Patterns"},{"depth":3,"value":"Identifying Phrase Structure"},{"depth":3,"value":"Building Recursive-Descent Parsers"},{"depth":3,"value":"Parser Construction Using a Grammar DSL"},{"depth":3,"value":"Tokenizing Sentences"},{"depth":3,"value":"Pattern 1: Mapping Grammars to Recursive-Descent Recognizers"},{"depth":4,"value":"Alternatives"},{"depth":4,"value":"Optional subrule <code class=\"language-text\">(T)?</code>"},{"depth":4,"value":"One or more <code class=\"language-text\">(...)+</code> subrules"},{"depth":4,"value":"Zero or more <code class=\"language-text\">(...)*</code> subrules"},{"depth":3,"value":"Pattern 2: LL(1) Recursive-Descent Lexer"},{"depth":3,"value":"Pattern 3: LL(1) Recursive-Descent Parser"},{"depth":4,"value":"Deterministic Parsing Decisions"},{"depth":3,"value":"Pattern 4: LL(k) Recursive-Descent Parser"},{"depth":2,"value":"Enhanced Parsing Patterns"},{"depth":3,"value":"Parsing with Arbitrary Lookahead"},{"depth":3,"value":"Parsing like a Pack Rat"},{"depth":3,"value":"Directing the Parse with Semantic Information"},{"depth":3,"value":"Pattern 5 Backtracking Parser"},{"depth":4,"value":"Dealing with Actions While Backtracking"},{"depth":3,"value":"Pattern 6 Memoizing Parser"},{"depth":3,"value":"Pattern 7 Predicated Parser"},{"depth":1,"value":"Analyzing Languages"},{"depth":2,"value":"Chapter 4 Building Intermediate Form Trees"},{"depth":3,"value":"Why We Build Trees"},{"depth":3,"value":"Building Abstract Syntax Trees"},{"depth":3,"value":"Representing Pseudo-operations in ASTs"},{"depth":3,"value":"Implementing ASTs in Java"},{"depth":3,"value":"Enforcing Tree Structure with the Type System"},{"depth":3,"value":"Constructing ASTs with ANTLR Grammars"},{"depth":3,"value":"Pattern 8: Parse Tree"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"},{"depth":3,"value":"Pattern 9: Homogeneous AST"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"},{"depth":3,"value":"Pattern 10: Normalized Heterogeneous AST"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"},{"depth":3,"value":"Pattern 11: Irregular Heterogeneous AST"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"},{"depth":2,"value":"Walking and Rewriting Trees"},{"depth":3,"value":"Walking Trees and Visitation Order"},{"depth":3,"value":"Pattern 12: Embedded Heterogeneous Tree Walker"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"},{"depth":3,"value":"Pattern 13: External Tree Visitor"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"},{"depth":4,"value":"Implementation"},{"depth":5,"value":"Visitor Switching on Node Type"},{"depth":5,"value":"Switching on the Token Type to Build Independent Visitors"},{"depth":3,"value":"Pattern 14: Tree Grammar"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"},{"depth":3,"value":"Pattern 15: Tree Pattern Matcher"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"},{"depth":2,"value":"Chapter 6 Tracking and Identifying Program Symbols"},{"depth":3,"value":"Collecting Information About Program Entities"},{"depth":3,"value":"Grouping Symbols into Scopes"},{"depth":3,"value":"Resolving Symbols"},{"depth":2,"value":"Chapter 7 Managing Symbol Tables for Data Aggregates"},{"depth":3,"value":"Building Scope Trees for Structs"},{"depth":3,"value":"Building Scope Trees for Classes"},{"depth":2,"value":"Chapter 8 Enforcing Static Typing Rules"},{"depth":1,"value":"Building Interpreters"},{"depth":2,"value":"Chapter 9 Building High-Level Interpreters"},{"depth":3,"value":"Designing High-Level Interpreter Memory Systems"},{"depth":3,"value":"Processing Instructions"},{"depth":2,"value":"Chapter 10 Building Bytecode Interpreters"},{"depth":3,"value":"Bytecode Machine Architecture"},{"depth":1,"value":"Translating and Generating Languages"},{"depth":2,"value":"Translating Computer Languages"},{"depth":3,"value":"Rule-Based Translation"},{"depth":3,"value":"Model-Driven Translation"},{"depth":3,"value":"Decoupling Input Model Traversal from Output Order"},{"depth":3,"value":"Organizing Translated Phrases into a Nested Model"},{"depth":3,"value":"Pattern 29: Syntax-Directed Translator"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"},{"depth":3,"value":"Pattern 30: Rule-Based Translator"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"},{"depth":3,"value":"Pattern 31: Target-Specific Generator Classes"},{"depth":4,"value":"Purpose"},{"depth":4,"value":"Discussion"}]}},"pageContext":{"id":"6bbd4217-bc21-53cb-be9e-32aaca14c2c5","previousPost":{"title":"Make as multi-paradigm language","slug":"/blog/make_paradigms"},"nextPost":{"title":"GCC, binutils and other developer tools","slug":"/blog/gcc_binutils"},"dates":{"created":1514309672,"modified":1647509701}}},
    "staticQueryHashes": []}